{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Day 2\n",
    "* Motivation\n",
    "* A working dataset\n",
    "* Importing data\n",
    "* Pandas data structures\n",
    "* Preprocessing data\n",
    "* Working with text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "* Data preparation is the un-sexy part of data science\n",
    "* But in many ways, the most important part\n",
    "* Choice of statistical estimator sometimes makes no practical difference\n",
    "    * \"More data beats better algorithms\" ([Domingos, 2012](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.715.3497&rep=rep1&type=pdf))\n",
    "* Small differences in preprocessing choices can ramify quickly\n",
    "    * E.g., variable smoothing or transformation, outlier removal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset\n",
    "* The City of Austin has an [Open Data Portal](https://data.austintexas.gov/) containing many interesting datasets\n",
    "* We'll use [outcome data](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238) from the Austin Animal Center\n",
    "* 65,605 outcomes between October 2013, and May 2018\n",
    "* Variables include the type, age, sex, breed, name, and color of the animal, plus the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some questions we can ask\n",
    "* What do people in Austin name their cats and dogs?\n",
    "* What are the most common dog and cat breeds processed at the animal center?\n",
    "* Do outcomes differ (e.g., euthanasia vs. adoption) for different breeds?\n",
    "    * Are purebred dogs more likely to be adopted than mixes?\n",
    "* Does animal size (e.g., big vs. small dogs) have any impact on outcomes?\n",
    "* How well can we predict the likely outcome given everything we know about an animal?\n",
    "* Do outcomes vary over time--e.g., by day of week, season, year, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Import all the things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's conventional when writing Python code to list all of the imports at the top of the file. That way you can quickly see exactly what modules and functions are being used, and it's easier to manage dependencies. In the following code cell, we'll import everything we're going to use in this notebook. Don't worry if you don't recognize some of the imported modules; we'll discuss most of them as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We'll consolidate all our imports at the top today\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# You may need to install the packages below. You can\n",
    "# probably just uncomment the next line and re-run the cell\n",
    "# !conda install beautifulsoup4 requests seaborn -y\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "# Disable annoying SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing data\n",
    "* Before we do anything else, we need to get our data into a usable form\n",
    "* Most commonly, data will come from a flat file\n",
    "* But sometimes we need to retrieve data from other sources\n",
    "* We'll do both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading in data with the standard library\n",
    "There are many ways to read in data into Python. Let's start by using the standard library--though this is just a learning exercise; in practice you would almost never have a good reason to do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = '../data/Austin_Animal_Center_Outcomes.csv'\n",
    "data = []  # Initialize an empty list to store the data\n",
    "\n",
    "# Loop over rows in the file. readlines() returns a list of\n",
    "# lines, so we can directly iterate it to get each line.\n",
    "for line in open(filename).readlines():\n",
    "    # Strip beginning or trailing whitespace from each line\n",
    "    line = line.strip()\n",
    "    # Split the comma-separated values into a list\n",
    "    line = line.split(',')\n",
    "\n",
    "    # Append the row to the dataset\n",
    "    data.append(line)\n",
    "\n",
    "print(\"Found {} rows.\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print the 1000th row to see what it looks like\n",
    "data[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The problem with approaches like the one above is that the data lack a tabular format, making it very hard to operate over rows or columns. We're much better off using the _pandas_ package to hold our data in a pandas DataFrame (DF)--a data structure that wraps around numpy arrays and is expressly designed to support a range of powerful operations over data. Reading a dataset into a pandas DF is very easy with the workhorse [read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [read_table()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html) methods. These methods take a large number of optional arguments that make it easy to read in almost any kind of orderly data represented in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reading data, the pandas way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas can read data from either a local file or a remote URL.\n",
    "# We'll default to reading directly from GitHub, but if you have\n",
    "# the Animal Center data available locally, comment out the next\n",
    "# line and uncomment the following one for faster loading.\n",
    "filename = \"https://raw.githubusercontent.com/tyarkoni/SSI2018/master/data/Austin_Animal_Center_Outcomes.csv\"\n",
    "# filename = \"../data/Austin_Animal_Center_Outcomes.csv\"\n",
    "\n",
    "# The workhorse data-reading method in pandas.\n",
    "# It accepts a LOT of optional arguments. You can see them\n",
    "# by executing ?pd.read_csv to bring up the documentation.\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calling head() on a DataFrame shows the first N rows.\n",
    "# Let's take a look and see what our data look like...\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other formats\n",
    "Pandas has built-in support for [reading from or to other common formats/sources](http://pandas.pydata.org/pandas-docs/stable/io.html):\n",
    "* Generic delimited text -- `read_table()`\n",
    "* Excel -- `read_excel()`\n",
    "* JSON -- `read_json()`\n",
    "* SQL -- `read_sql()`\n",
    "* Stata -- `read_stata()`\n",
    "* SAS (XPORT or SAS7BDAT) -- `read_sas()`\n",
    "* etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scraping data\n",
    "* What if we want to add some data to our dataset?\n",
    "* It would be nice if we had height and weight estimates for dog breeds\n",
    "    * Are there different outcomes for bigger vs. smaller dogs?\n",
    "* We track down a website that has some [breed information](https://www.wisdompanel.com/explore-breed/breeds-detected/)\n",
    "* Now we need to \"scrape\" that data and get it into Python/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# To make our requests realistic, we'll pretend we're using Chrome on a Mac.\n",
    "# Without this, many websites will reject our request.\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2227.1 Safari/537.36'}\n",
    "\n",
    "# The page listing all the breeds available from Wisdom Panel\n",
    "url = \"https://www.wisdompanel.com/explore-breed/breeds-detected/\"\n",
    "\n",
    "# Get the HTML contents of the page\n",
    "html = requests.get(url).text\n",
    "\n",
    "# Create a BeautifulSoup document\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract links to all breeds with BeautifulSoup\n",
    "breed_links = soup.select('div.breeds-accordion-inn a')\n",
    "\n",
    "breed_links = [\"http://www.wisdompanel.com\" + a['href'] for a in breed_links]\n",
    "print(\"Retrieving data for {} breeds.\".format(len(breed_links)))\n",
    "\n",
    "breed_data = []\n",
    "\n",
    "# Note: I've deliberately limited the scraping to the first 3 breeds.\n",
    "# You can run this to see how it works, but there's no point in having\n",
    "# everyone scrape all 200+ dog breeds, since I've made the scraped data\n",
    "# available in the GitHub repo, and we'll use that below.\n",
    "for breed in breed_links[:3]:\n",
    "\n",
    "    try:\n",
    "        breed_html = requests.get(breed, headers=headers).text\n",
    "\n",
    "        # Use regular expressions to extract name, height, and weight. It would probably be\n",
    "        # easier to use BeautifulSoup for this, but we want to illustrate the use of regexes.\n",
    "        \n",
    "        # Breed name shows up prominently at the top\n",
    "        name = re.search('content=\"(.*?)\\s\\|\\sWisdom Panel\"', breed_html, re.DOTALL).group(1)\n",
    "\n",
    "        # For height and weight, first get the enclosing div with BeautifulSoup\n",
    "        breed_soup = BeautifulSoup(breed_html, 'html.parser')\n",
    "        trait_divs = breed_soup.select('div.appearance-row')\n",
    "\n",
    "        # We have multiple traits, so let's store them in a dictionary\n",
    "        traits = {}\n",
    "        \n",
    "        # Now loop over each detected trait and parse into key-value pairs\n",
    "        # using regular expressions\n",
    "        for div in trait_divs:\n",
    "            div_text = div.text.strip()\n",
    "            pattern = '(.*?):\\s+(\\d+)[\\s-]+(\\d+)'\n",
    "            matches = re.search(pattern, div_text, re.DOTALL)\n",
    "            # There are three matched groups. We can assign them\n",
    "            # to three variables in parallel in Python.\n",
    "            trait_name, min_val, max_val = matches.groups()\n",
    "            # Store each trait in our traits dict\n",
    "            traits[trait_name] = [min_val, max_val]\n",
    "            \n",
    "        print(\"Data extracted for breed '{}':\".format(name), traits)\n",
    "        \n",
    "        # Consolidate all of the extracted values in a single row.\n",
    "        # We'll only keep the weight range for pets, not for show dogs.\n",
    "        breed_row = [name] + traits['Height'] + traits['Weight (pet)']\n",
    "#         print(\"Retrieved breed data: \", breed_row)\n",
    "        breed_data.append(breed_row)\n",
    "        \n",
    "    # Some breed pages have formatting errors, so we'll just skip them\n",
    "    except Exception as e:\n",
    "        print(\"An unspecified error occurred while retrieving data for a breed.\")\n",
    "        pass\n",
    "\n",
    "# Put the data in a pandas DataFrame\n",
    "col_names = ['breed_name', 'min_height', 'max_height', 'min_weight', 'max_weight']\n",
    "breed_data = pd.DataFrame(breed_data, columns=col_names)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join('..', 'data')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the breed data locally for re-use\n",
    "output_file = os.path.join(output_dir, 'breed_data.csv')\n",
    "breed_data.to_csv(output_file, index=False, encoding='utf8'),\n",
    "print(\"Successfully saved breed data to {}.\".format(output_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The scraped breed data\n",
    "\n",
    "We'll come back later to the breed data we just scraped and saved. But for now, let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use the already-scraped data in the repository,\n",
    "# to prevent hammering Wisdom Panel's server.\n",
    "breed_data = pd.read_csv('https://raw.githubusercontent.com/tyarkoni/SSI2018/master/data/breed_data.csv')\n",
    "\n",
    "breed_data#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regular expressions\n",
    "* The heavy lifting in the previous scraping code is done with regular expressions\n",
    "* A powerful system for detecting and capturing patterns in text\n",
    "* One of the most underutilized features of programming languages\n",
    "    * Not just in Python! R, SAS, Stata, and most other language support regex\n",
    "* A short regular expression can replace dozens of lines of string-processing code\n",
    "* Lots of good tutorials ([1](https://developers.google.com/edu/python/regular-expressions), [2](http://regexone.com/references/python), [3](http://www.learnpython.org/en/Regular_Expressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A snippet of HTML from the Alaskan Malamute's breed\n",
    "# website on wisdompanel.com. We want to extract just the\n",
    "# height information.\n",
    "text = '''\n",
    "<div class=\"appearance-row\">\n",
    "    <span class=\"label\">Height:</span>\n",
    "    <span class=\"value\">23 - 28 in</span>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# Extract with regular expressions. In this case,\n",
    "# We're looking to capture two groups of numbers,\n",
    "# corresponding to the minimum and maximum height.\n",
    "# We can use the \"value\" class name as a useful\n",
    "# marker to indicate where to start scanning for\n",
    "# matches. \n",
    "pattern = 'value\"\\>(\\d+)[\\s-]+(\\d+)'\n",
    "\n",
    "# Search for pattern within text.\n",
    "# The re.DOTALL flag indicates that we want to match\n",
    "# across multiple lines of text, including newlines.\n",
    "matches = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "# We should have two matching groups, so we can\n",
    "# assign them to two variables in parallel.\n",
    "min_height, max_height = matches.groups()\n",
    "\n",
    "print(min_height, max_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some simple regex examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define a string we'll apply regexes to\n",
    "text = \"A few additional examples (maybe 4 or 5) illustrating the power of regular expressions!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# All text beginning with 'add' and ending in 'power'.\n",
    "# The period ('.') matches any character; the asterisk indicates\n",
    "# the pattern should be matched as many times as possible.\n",
    "re.search('(add.*power)',  text).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    " # All numbers in the text. Note the use of findall()\n",
    "# instead of search() or match().\n",
    "re.findall('(\\d+)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# All words between 5 and 8 characters in length\n",
    "re.findall(r'\\b(\\w{5,8})\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas data structures\n",
    "* Provides functionality similar to data frames in R\n",
    "* Two main data structures: Series and DataFrames\n",
    "    * DataFrames: 2-dimensional numpy array with labeled rows and columns\n",
    "    * Series: 1-dimensional numpy array with labeled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a Series from a numpy array and index labels\n",
    "a = np.arange(3, 8)\n",
    "labels = ['apple', 'banana', 'orange', 'pear', 'grapes']\n",
    "b = pd.Series(a, index=labels)\n",
    "\n",
    "# Let's take a look...\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Unlike numpy arrays, we can now refer to elements by label.\n",
    "# The syntax is similar to dictionary indexing. You can also\n",
    "# treat labels like attributes (e.g., b.pear), but this runs\n",
    "# the risk of collisions and should be avoided.\n",
    "b['pear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can always retrieve the underlying numpy array with .values\n",
    "b.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Many numpy operations work as expected, including slicing\n",
    "b[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The pandas DataFrame\n",
    "* The workhorse of data analysis in pandas\n",
    "* A container of multiple aligned Series\n",
    "* Heterogeneous: a DF's Series can have different dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Indexing pandas DataFrames\n",
    "* pandas DFs support [flexible indexing](http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing) by labels and/or indices\n",
    "    * A common gotcha: R-style indexing won't work\n",
    "    * Be explicit about whether you're using integer or label indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's remind ourselves what the data look like\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This won't work!\n",
    "data[0, 'Animal Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# .ix supports mixed positional and label-based access.\n",
    "# But note that this  is deprecated! Can anyone guess\n",
    "# why this type of indexing could cause trouble?\n",
    "data.ix[0, 'Animal Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Returns the entire column\n",
    "data['Animal Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Position-based selection; returns all of rows 2 - 5\n",
    "data.iloc[2:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Returns rows 2 - 5, columns 2 and 7\n",
    "data.iloc[2:5, [2, 7]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Label-based indexing; e quivalent to data['Animal Type']\n",
    "# in this case \n",
    "data.loc[:, 'Animal Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A quick summary of the data\n",
    "We'll cover descriptive analyses in more detail tomorrow, but for now, a basic summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing our data\n",
    "Much of what data scientists do involves cleaning and preprocessing data:\n",
    "* Handling missing or invalid values\n",
    "* Extracting usable information from messy strings\n",
    "* Transforming/normalizing variables and variable names\n",
    "* Filtering redundant or bad data\n",
    "* Merging with other datasets\n",
    "* Etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some examples\n",
    "* There's far too much to cover in 2 hours, so we'll cherry-pick useful examples\n",
    "* There are many excellent tutorials and guides (e.g., [1](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/), [2](http://synesthesiam.com/posts/an-introduction-to-pandas.html), [3](https://github.com/fonnesbeck/statistical-analysis-python-tutorial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Renaming/replacing\n",
    "The column names in our dataset are kind of clunky. Let's make it a bit easier to work with the dataset by standardizing and shortening them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This is no good\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of old names --> new names.\n",
    "# Notice that we don't need to provide replacements\n",
    "# for _all_ the columns,  just the ones we want to\n",
    "# replace. An alternative approach would be to\n",
    "# directly set data.columns to a list of the\n",
    "# desired new column names.\n",
    "replacements = {\n",
    "    'Animal ID': 'id',\n",
    "    'DateTime': 'datetime',\n",
    "    'Outcome Type': 'outcome',\n",
    "    'Outcome Subtype': 'outcome_subtype',\n",
    "    'Animal Type': 'animal',\n",
    "    # We append these with '_string' because we're going\n",
    "    # to use the names 'sex' and 'age' later for\n",
    "    # extracted variables, so this avoids confusion.\n",
    "    'Sex upon Outcome': 'sex_string',\n",
    "    'Age upon Outcome': 'age_string',\n",
    "}\n",
    "\n",
    "# We can apply the rename function to either row names\n",
    "# or column names. Here we're doing columns.\n",
    "data = data.rename(columns=replacements)\n",
    "\n",
    "# Let's also make sure all column names are in lowercase.\n",
    "# We're using a Python idiom called a \"list comprehension\",\n",
    "# which is basically a more compact way to write a for-loop.\n",
    "data.columns = [c.lower() for c in data.columns]\n",
    "\n",
    "# # # The above list comprehension is functionally equivalent\n",
    "# # # to the following for-loop:\n",
    "# columns = []\n",
    "# for col in data.columns:\n",
    "#     lower_col = col.lower()\n",
    "#     columns.append(lower_col)\n",
    "# data.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Working with time series data\n",
    "* The datetime column in our dataset is currently represented as a string\n",
    "* We want to treat it as a datetime, so we can easily extract information\n",
    "* Let's convert its time to datetime and pull out some useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# These are actually strings/objects, not datetimes!\n",
    "print(data['datetime'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the datetime column to actual datetimes.\n",
    "# Note that this operation can take a bit of time when\n",
    "# working with large datasets, because pandas has to\n",
    "# inspect each individual string and try to make sense\n",
    "# of it. There are other faster ways to go about this,\n",
    "# but they would require us to explicitly specify the\n",
    "# format the datetimes are in, which is a hassle, and\n",
    "# might run into problems if there are multiple formats.\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    " \n",
    "# Now we're working with datetimes\n",
    "print(data['datetime'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can easily extract some useful time-related variables for later use (e.g., year, day of week, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Datetime-related functions are accessed through the .dt attribute\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['day'] = data['datetime'].dt.weekday\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parsing text data\n",
    "* Many datasets contain nominal/categorical variables that need to be recoded as numeric variables\n",
    "* Three common strategies:\n",
    "    * Transform a nominal variable into a continuous one\n",
    "    * Break down a compound nominal label into constituent numeric variables\n",
    "    * \"Dummy-code\" a nominal variable as a set of binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representing animal age with numeric values\n",
    "The age column in our DF has nasty values like \"3 years\" and \"7 months\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data['age_string'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Converting descriptive age strings to months of age\n",
    "How do we convert these strings to numeric values we can operate on?\n",
    "\n",
    "We can take advantage of pandas' apply() functionality, which applies arbitrary methods to each value in a Series (or each column or row in a DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def extract_months(value):\n",
    "    ''' Extract and return age in months from string values \n",
    "    \"3 years\" or \"7 months\".\n",
    "    \n",
    "    Note: this code won't work as written, because some of\n",
    "    the values passed in are invalid.\n",
    "    \n",
    "    In-class exercise: we modify the code to return the\n",
    "    special value np.nan (numpy's representation of not-a-number)\n",
    "    whenever an invalid value is encountered.\n",
    "    '''\n",
    "    number, unit = value.split(' ')\n",
    "    number = int(number)\n",
    "\n",
    "    if unit.startswith('year'):\n",
    "        number = number * 12\n",
    "\n",
    "    return number\n",
    "\n",
    "data['age'] = data['age_string'].apply(extract_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Notice any problems in the output? (Hint: week)\n",
    "# As an exercise, you can fix this by tweaking the\n",
    "# extract_months() method we wrote above and rerunning\n",
    "# the previous code block. \n",
    "data[['age_string', 'age']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Just for kicks...\n",
    "We're not ready to cover plotting just yet, but since we now have our first continuous variable, let's reward ourselves by plotting something nice to look at..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x='animal', y='age', data=data, hue='outcome')\n",
    "plt.gcf().set_size_inches((20, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting data from the sex column\n",
    "* The original column combines sex and sterilization status into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data['sex_string'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's recode this as two binary variables: sex (male/female) and sterilized (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: The code below will fail the first time you run it. Why?\n",
    "# Because there's a single NaN value that messes up the indexing.\n",
    "# These kinds of errors are pervasive in real-world datasets.\n",
    "# We need to recode the NaN--which we can do by setting it to\n",
    "# 'Unknown', just like the other 3,625 Unknown values. To do\n",
    "# that, uncomment the next line and re-run.\n",
    "\n",
    "# data['sex_string'] = data['sex_string'].fillna('Unknown')\n",
    "\n",
    "# Initialize a new column with NaN as the default value (for unknown sex)\n",
    "data['sex'] = np.nan\n",
    "\n",
    "# Update the column with values for M (0) and F (1)\n",
    "inds = data['sex_string'].str.contains('Male')\n",
    "data['sex'][inds] = 0\n",
    "data['sex'][data['sex_string'].str.contains('Female')] = 1\n",
    "\n",
    "# Now do the same kind of thing for sterilization\n",
    "data['sterilized'] = 1\n",
    "data['sterilized'][data['sex_string'].str.contains('Intact')] = 0\n",
    "data['sterilized'][data['sex_string'].str.contains('Unknown')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data[['sex_string', 'sex', 'sterilized']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: an alternative (and simpler) way to do the above would have been to use pandas' .replace() method. As an exercise, rewrite the code above to achieve the same goal using .replace(). (Hint: create a dictionary of values to find and replace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dummy-coded variables\n",
    "* Categorical variables with N levels often need to be modeled as N binary indicators\n",
    "* E.g., day-of-week is coded 0 through 6, but should be modeled as 6 or 7 contrasts\n",
    "* pandas makes this easy with get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can also specify drop_first=True (on pandas >= 0.18) in order to\n",
    "# use N - 1 columns instead of N (e.g., to reduce collinearity).\n",
    "# It's also often useful to pass a prefix argument, which is prepended\n",
    "# to all the level names.\n",
    "pd.get_dummies(data['outcome'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filtering data\n",
    "* It's very common to want to filter out certain rows/columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropping uncommon outcomes\n",
    "* Let's take a look at all of the different outcomes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show the frequencies of all of the different  outcomes\n",
    "data['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Some outcomes in the dataset are too rare to merit analyzing\n",
    "    * We already saw how extra levels clutter up our plots and understanding\n",
    "* Let's impose a minimum cut-off of 1000 occurrences for outcomes to be kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A naive approach\n",
    "How would we solve this problem in pure Python, without pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 1: the most naive approach, without\n",
    "# taking advantage of anything pandas has to offer.\n",
    "# This is slower and involves much more code than\n",
    "# the pandas-based solutions, but it works just fine.\n",
    "# To illustrate the logic behind what we're doing,\n",
    "# we use only native Python here, and no fancy idioms.\n",
    "# The other approaches do similar things under the\n",
    "# hood, but take advantage of many optimizations and\n",
    "# functions written in C to greatly speed things up.\n",
    "\n",
    "# Get the raw outcome values as a list\n",
    "values = data['outcome'].tolist()\n",
    "\n",
    "# Initialize a dictionary to store running counts of\n",
    "# all the outcomes.\n",
    "counts = {}\n",
    "\n",
    "# Loop over values and increment the appropriate\n",
    "# outcome's counter.\n",
    "for v in values:\n",
    "    if v not in counts:\n",
    "        counts[v] = 0\n",
    "    counts[v] += 1\n",
    "\n",
    "# Select the outcomes we want to keep--only those\n",
    "# above 1000 occurrences.\n",
    "min_count = 1000\n",
    "\n",
    "# Initialize a list to store only the valid outcomes\n",
    "valid_outcomes = []\n",
    "\n",
    "# Loop over the outcome-count pairs in the dictionary\n",
    "# and keep only those above the threshold.\n",
    "for outcome, count in counts.items():\n",
    "    if count >= min_count:\n",
    "        valid_outcomes.append(outcome)\n",
    "\n",
    "# Identify the valid rows in the pandas DataFrame\n",
    "rows_to_keep = []\n",
    "for i, row in data.iterrows():\n",
    "    if row['outcome'] in valid_outcomes:\n",
    "        rows_to_keep.append(row)\n",
    "\n",
    "# Turn this list of Series back into a DataFrame\n",
    "filtered_data = pd.DataFrame(rows_to_keep)\n",
    "\n",
    "# Count the number of rows we have left and print them out\n",
    "n_rows = filtered_data.shape[0]\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(n_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A better way\n",
    "* The pure Python approach works, but it's slow and very verbose\n",
    "* We can do much better with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: We do essentially the same thing, but now we\n",
    "# take advantage of various built-in pandas methods.\n",
    "# Notice how much shorter--and faster!--the solution is.\n",
    "\n",
    "# Count the number of occurrences of each outcome\n",
    "counts = data['outcome'].value_counts()\n",
    "\n",
    "# # Keep only those above a threshold--say 1000\n",
    "valid_inds = counts >= 1000\n",
    "valid_outcomes = counts[valid_inds].index\n",
    "\n",
    "# Now select only dataset rows with those outcomes\n",
    "valid_rows = data['outcome'].isin(valid_outcomes)\n",
    "filtered_data = data[valid_rows]\n",
    "\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(filtered_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### An even better way\n",
    "* That was much better. But we can do this even _more_ efficiently...\n",
    "* We'll use the split-apply-combine pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The [split-apply-combine](https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf) pattern\n",
    "* A very common data processing strategy\n",
    "    * Split the dataset into groups\n",
    "    * Apply some operation(s) to each group\n",
    "    * (Optionally) combine back into one dataset\n",
    "* pandas provides powerful and fast tools for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Calculate the mean age of each animal species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A simple example calculate the mean age of each animal species\n",
    "\n",
    "# Split the dataset on animal. This creates a pandas GroupBy object.\n",
    "groups = data.groupby('animal')\n",
    "\n",
    "# pandas provides many built-in operations on groups--e.g., mean, count, etc.\n",
    "# see http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-object-attributes\n",
    "groups['age'].mean()\n",
    "\n",
    "# The above line can also be more explicitly written as:\n",
    "# groups['age'].aggregate('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count the number of animals of each species that are older than 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can apply arbitrary methods to each group.\n",
    "# Here we use a small \"lambda function\" (comparable to\n",
    "# an anonymous function in JavaScript, R, or Matlab)\n",
    "# to count the number of animals of each species\n",
    "# that are older than 12 months.\n",
    "groups = data.groupby(['animal', 'outcome'])\n",
    "groups['age'].apply(lambda x: (x > 12).sum())\n",
    "\n",
    "# A functionally equivalent, but more explicit way of\n",
    "# doing the above would be:\n",
    "\n",
    "# def count_animals_older_than_12_months(x):\n",
    "#     return (x > 12).sum()\n",
    "\n",
    "# groups['age'].apply(count_animals_older_than_12_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Filtering uncommon outcomes: the one-line solution\n",
    "With the above pattern in mind, we can now turn back to our outcome-filtering problem and solve it in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 3: Use groupby() and a short lambda function.\n",
    "\n",
    "filtered_data = data.groupby('outcome').filter(lambda x: len(x) >= 1000)\n",
    "\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(filtered_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Filtering animals for only the last outcome\n",
    "* Some animals have multiple outcomes in the dataset\n",
    "* Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Count number of outcomes per animal\n",
    "outcome_counts = data.groupby('id')['outcome'].count()\n",
    "\n",
    "# How many animals with more than one outcome?\n",
    "outcome_counts[outcome_counts > 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We probably want to analyze only the _last_ outcome for each animal\n",
    "* This also becomes easy with the groupby strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Recall that we have a datetime column we can sort on,\n",
    "# to put all outcomes in chronological order.\n",
    "data = data.sort_values('datetime')\n",
    "\n",
    "# Group on animal and then use the built-in last() function\n",
    "filtered_data = data.groupby('id').last()\n",
    "\n",
    "# Note the handy comma injection the Python format method provides\n",
    "print(\"Before filtering: {:,} rows\".format(len(data)))\n",
    "print(\"After filtering: {:,} rows\".format(len(filtered_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Merging datasets\n",
    "* It's very common to need to combine data from different sources\n",
    "* Remember the breed data we scraped?\n",
    "* Let's merge it into our main DataFrame\n",
    "    * Will allow us to test whether genetic group, weight, or height matter\n",
    "* This is a more complicated operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The problem\n",
    "* We want to merge or 'join' our outcome data with the breed data\n",
    "* The obvious column to join on here is the breed name\n",
    "* But the names don't line up! Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data['breed'].value_counts().index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(breed_data['breed_name'].value_counts().index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's what we'll do:\n",
    "* Drop terms like \"mix\", \"shorthair\" and \"longhair\" that don't occur in the official names\n",
    "* For mixed breeds in our outcome data, we'll only use the first breed\n",
    "* For the most common breeds, we'll manually recode names so that they align\n",
    "* Note that all these steps are imperfect; our data will be noisy\n",
    "    * When munging data, we often need to decide how good is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Restrict analysis only to dogs, since that's all we have breed info for.\n",
    "# Notice our use of the query function, which enables us to perform SQL-like\n",
    "# queries over DataFrames.\n",
    "dogs = data.query('animal == \"Dog\"')\n",
    "\n",
    "# Let's prepare a 'merge_key' column in our outcome data that should line up\n",
    "# as well as possible with the breed_name column in the breed_data DataFrame.\n",
    "# We'll start by dropping all occurrences of 'Mix', 'Shorthair', etc.\n",
    "dogs['merge_key'] = dogs['breed'].replace([' Mix', ' Shorthair', ' Longhair'], '', regex=True)\n",
    "\n",
    "# Next, we'll keep only the first breed in cases where there are multiple given.\n",
    "# For example, \"Beagle/Labrador Retriever\" would become just \"Beagle\".\n",
    "dogs['merge_key'] = dogs['merge_key'].str.split('/').str.get(0)\n",
    "\n",
    "# Manually map the most common breeds that don't align. I've cheated here\n",
    "# and done a bit of extra work ahead of time to figure out what these are.\n",
    "replacements = {\n",
    "    \"Pit Bull\": \"American Staffordshire Terrier\",\n",
    "    \"Staffordshire\": \"Staffordshire Bull Terrier\",\n",
    "    \"German Shepherd\": \"German Shepherd Dog\",\n",
    "    \"Anatol Shepherd\": \"Anatolian Shepherd Dog\",\n",
    "    \"Australian Shepherd\": \"Australian Shepherd Dog\",\n",
    "    \"Catahoula\": \"Catahoula Leopard Dog\",\n",
    "    \"American Pit Bull Terrier\": \"American Staffordshire Terrier\", \n",
    "    \"Doberman Pinsch\": \"Doberman Pinscher\",  \n",
    "}\n",
    "\n",
    "# Apply the replacements\n",
    "dogs['merge_key'] = dogs['merge_key'].replace(replacements)\n",
    "\n",
    "# Finally, we're in a position to merge our data.\n",
    "# We have to tell pandas which columns to use in each of the two datasets.\n",
    "# We can also specify how we want the join to work ('left', 'outer',\n",
    "# 'inner', etc.). The default is left, meaning that the output DataFrame\n",
    "# will have the same number of rows as the first input DataFrame (dogs).\n",
    "dogs = dogs.merge(breed_data, left_on='merge_key', right_on='breed_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dogs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your (optional) assignment for tomorrow\n",
    "* We extracted a numeric representation of several categorical variables\n",
    "    * We could extract many more\n",
    "* Some more variables we'll probably look at when we turn to fitting models:\n",
    "    * Animal color\n",
    "    * Whether the animal is a purebreed or mix\n",
    "    * Whether the outcome resulted in an animal's death (e.g., Died + Euthanasia)\n",
    "* Extract anything else you think might be useful"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
